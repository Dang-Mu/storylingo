#!/usr/bin/env python3
"""
pos_split_results.jsonì„ ê¸°ë°˜ìœ¼ë¡œ data/ í´ë”ì˜ ìŠ¤í† ë¦¬ íŒŒì¼ë“¤ì„ í™•ì¥í•©ë‹ˆë‹¤.
- partOfSpeech í•„ë“œ ì¶”ê°€
- wrongAnswers í•„ë“œ ì¶”ê°€ (ê°™ì€ í’ˆì‚¬ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤)
"""

import json
import os
from pathlib import Path
from typing import Dict, List, Optional, Set
import re

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬
ROOT_DIR = Path(__file__).parent.parent
POS_RESULTS_FILE = ROOT_DIR / "utils" / "pos_split_results.json"
DATA_DIR = ROOT_DIR / "data"
PUBLIC_DATA_DIR = ROOT_DIR / "public" / "data"

def normalize_word(word: str) -> str:
    """ë‹¨ì–´ë¥¼ ì •ê·œí™” (ì†Œë¬¸ì, êµ¬ë‘ì  ì œê±°)"""
    # êµ¬ë‘ì  ì œê±°
    word = re.sub(r'[^\w\s]', '', word)
    return word.lower().strip()

def find_word_pos(word: str, pos_breakdown: Dict[str, List[str]]) -> Optional[str]:
    """ë‹¨ì–´ê°€ ì–´ë–¤ í’ˆì‚¬ì¸ì§€ ì°¾ê¸°"""
    normalized_word = normalize_word(word)
    
    # ê° í’ˆì‚¬ë³„ë¡œ ê²€ìƒ‰
    for pos, words in pos_breakdown.items():
        for w in words:
            if normalize_word(w) == normalized_word:
                # ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ëŠ” í’ˆì‚¬ íƒ€ì…ìœ¼ë¡œ ë§¤í•‘
                if pos == "noun":
                    return "noun"
                elif pos == "verb":
                    return "verb"
                elif pos == "adjective":
                    return "adjective"
                elif pos == "adverb":
                    return "adverb"
    
    return None

def get_wrong_answers(target_word: str, pos: Optional[str], pos_breakdown: Dict[str, List[str]], all_sentences: List[Dict]) -> List[str]:
    """ê°™ì€ í’ˆì‚¬ì˜ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ì„ ì˜¤ë‹µìœ¼ë¡œ ìƒì„±"""
    if not pos:
        return []
    
    wrong_answers = []
    normalized_target = normalize_word(target_word)
    
    # í˜„ì¬ ìŠ¤í† ë¦¬ì˜ ëª¨ë“  ë¬¸ì¥ì—ì„œ ê°™ì€ í’ˆì‚¬ì˜ ë‹¨ì–´ë“¤ ìˆ˜ì§‘
    candidate_words: Set[str] = set()
    
    for sentence_data in all_sentences:
        pos_breakdown = sentence_data.get("pos_breakdown", {})
        pos_words = pos_breakdown.get(pos, [])
        
        for word in pos_words:
            normalized = normalize_word(word)
            if normalized != normalized_target and normalized and len(normalized) > 1:
                candidate_words.add(normalized)
    
    # í›„ë³´ ë‹¨ì–´ë“¤ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜í•˜ê³  ì •ë ¬ (ì¼ê´€ì„±ì„ ìœ„í•´)
    candidate_list = sorted(list(candidate_words))
    
    # ì •ë‹µê³¼ ë‹¤ë¥¸ ë‹¨ì–´ë“¤ ì¤‘ì—ì„œ 3ê°œ ì„ íƒ
    for word in candidate_list:
        if len(wrong_answers) >= 3:
            break
        if word != normalized_target:
            wrong_answers.append(word)
    
    # 3ê°œê°€ ì•ˆ ë˜ë©´ ê¸°ë³¸ ì˜¤ë‹µ ì¶”ê°€
    default_wrongs = {
        "noun": ["thing", "place", "person", "object", "item"],
        "verb": ["do", "make", "get", "take", "go"],
        "adjective": ["good", "bad", "big", "small", "new"],
        "adverb": ["well", "badly", "quickly", "slowly", "carefully"]
    }
    
    if len(wrong_answers) < 3 and pos in default_wrongs:
        for word in default_wrongs[pos]:
            if len(wrong_answers) >= 3:
                break
            normalized = normalize_word(word)
            if normalized != normalized_target and normalized not in wrong_answers:
                wrong_answers.append(normalized)
    
    return wrong_answers[:3]

def enhance_story_file(story_filename: str, pos_data: Dict) -> bool:
    """ë‹¨ì¼ ìŠ¤í† ë¦¬ íŒŒì¼ì„ í™•ì¥"""
    story_file = DATA_DIR / story_filename
    public_story_file = PUBLIC_DATA_DIR / story_filename
    
    if not story_file.exists():
        print(f"âš ï¸  {story_filename} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    # JSON íŒŒì¼ ì½ê¸°
    try:
        with open(story_file, 'r', encoding='utf-8') as f:
            story_data = json.load(f)
    except Exception as e:
        print(f"âŒ {story_filename} ì½ê¸° ì‹¤íŒ¨: {e}")
        return False
    
    # pos_split_resultsì—ì„œ í•´ë‹¹ ìŠ¤í† ë¦¬ ë°ì´í„° ì°¾ê¸°
    if story_filename not in pos_data:
        print(f"âš ï¸  {story_filename}ì— ëŒ€í•œ í’ˆì‚¬ ë¶„ì„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    analyzed_data = pos_data[story_filename]
    analyzed_sentences = analyzed_data.get("analyzed_sentences", [])
    
    # ë¬¸ì¥ ë§¤ì¹­ ë° í™•ì¥
    enhanced_count = 0
    
    for sentence in story_data.get("sentences", []):
        english = sentence.get("english", "")
        target_word = sentence.get("targetWordEnglish", "")
        
        if not target_word:
            continue
        
        # analyzed_sentencesì—ì„œ ë§¤ì¹­ë˜ëŠ” ë¬¸ì¥ ì°¾ê¸°
        matched_analysis = None
        for analyzed in analyzed_sentences:
            if analyzed.get("original_english", "").strip() == english.strip():
                matched_analysis = analyzed
                break
        
        if not matched_analysis:
            # ì •í™•íˆ ì¼ì¹˜í•˜ì§€ ì•Šìœ¼ë©´ ë¶€ë¶„ ë§¤ì¹­ ì‹œë„
            for analyzed in analyzed_sentences:
                analyzed_english = analyzed.get("original_english", "")
                # ë¬¸ì¥ì˜ ì£¼ìš” ë¶€ë¶„ì´ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸
                if target_word.lower() in analyzed_english.lower():
                    matched_analysis = analyzed
                    break
        
        if matched_analysis:
            pos_breakdown = matched_analysis.get("pos_breakdown", {})
            
            # í’ˆì‚¬ ì°¾ê¸°
            pos = find_word_pos(target_word, pos_breakdown)
            if pos:
                sentence["partOfSpeech"] = pos
                enhanced_count += 1
            
            # ì˜¤ë‹µ ìƒì„±
            wrong_answers = get_wrong_answers(
                target_word, 
                pos, 
                pos_breakdown,
                analyzed_sentences
            )
            
            if wrong_answers:
                sentence["wrongAnswers"] = wrong_answers
    
    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ì €ì¥
    try:
        with open(story_file, 'w', encoding='utf-8') as f:
            json.dump(story_data, f, ensure_ascii=False, indent=2)
        
        # public/data í´ë”ì—ë„ ë³µì‚¬
        if public_story_file.parent.exists():
            with open(public_story_file, 'w', encoding='utf-8') as f:
                json.dump(story_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… {story_filename}: {enhanced_count}ê°œ ë¬¸ì¥ í™•ì¥ ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ {story_filename} ì €ì¥ ì‹¤íŒ¨: {e}")
        return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ“š ìŠ¤í† ë¦¬ íŒŒì¼ í™•ì¥ ì‹œì‘...\n")
    
    # pos_split_results.json ì½ê¸°
    if not POS_RESULTS_FILE.exists():
        print(f"âŒ {POS_RESULTS_FILE} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    try:
        with open(POS_RESULTS_FILE, 'r', encoding='utf-8') as f:
            pos_data = json.load(f)
    except Exception as e:
        print(f"âŒ pos_split_results.json ì½ê¸° ì‹¤íŒ¨: {e}")
        return
    
    # data í´ë”ì˜ ëª¨ë“  JSON íŒŒì¼ ì²˜ë¦¬
    story_files = sorted(DATA_DIR.glob("*.json"))
    
    if not story_files:
        print("âš ï¸  data í´ë”ì— JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    success_count = 0
    total_count = len(story_files)
    
    for story_file in story_files:
        filename = story_file.name
        if enhance_story_file(filename, pos_data):
            success_count += 1
        print()
    
    print(f"âœ¨ ì™„ë£Œ: {success_count}/{total_count}ê°œ íŒŒì¼ í™•ì¥ ì„±ê³µ")

if __name__ == "__main__":
    main()
